{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is about developing few-shot learning, model fine-tuning, and templates for guiding LLM outputs.\n",
    "\n",
    "### Leveraging SFT(supervised fine tuning) to develop and deploy a AI Agent to generate sql query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load training dataset spider from Hugging Face - with 100 in train and 10 in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, validation_dataset  = load_dataset('xlangai/spider', split=['train[0:100]', 'validation[0:10]'])\n",
    "client = OpenAI()\n",
    "llm_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_finetuning_dataset():\n",
    "\tsystem_prompt = \"Spider knows SQL\"\n",
    "\tdataset_dict = {\"training\": training_dataset, \"validation\": validation_dataset}\n",
    "\n",
    "\tdef format_dataset(dataset_dict):\n",
    "\t\tformatted_data = []\n",
    "\t\tfor dataset_name, dataset in dataset_dict.items():\n",
    "\t\t\tfor row in dataset:\n",
    "\t\t\t\tquestion = row['question']\n",
    "\t\t\t\tquery = row['query']\n",
    "\t\t\t\tmessage = {\n",
    "\t\t\t\t\t\"messages\": [\n",
    "\t\t\t\t\t\t{\"role\": \"system\", \"content\": system_prompt},\n",
    "\t\t\t\t\t\t{\"role\": \"user\", \"content\": question},\n",
    "\t\t\t\t\t\t{\"role\": \"assistant\", \"content\": query}\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t}\n",
    "\t\t\t\tformatted_data.append(message)\n",
    "\t\t\tformatted_df = pd.DataFrame(formatted_data)\n",
    "\t\t\tformatted_df.to_json(f\"{dataset_name}_dataset.jsonl\", orient='records', lines=True)\n",
    "\n",
    "\tformat_dataset(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(system_prompt,prompt, model=llm_model):\n",
    "\tchat_completion = client.chat.completions.create(\n",
    "\t\tmessages=[\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"system\",\n",
    "\t\t\t\t\"content\": system_prompt\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\"content\": prompt,\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\tmodel=model,\n",
    "\t)\n",
    "\treturn chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance - exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformating_data(data):\n",
    "\n",
    "\tdata['query'] = data['query'].apply(lambda x: re.sub(r'\\s+', ' ',x))\n",
    "\tdata['predicted_query'] = data['predicted_query'].apply(lambda x: re.sub(r'\\s+', ' ',x))\n",
    "\tdata['query'] = data['query'].str.lower()\n",
    "\tdata['predicted_query'] = data['predicted_query'].str.lower()\n",
    "\treturn data\n",
    "\n",
    "def evaluate_dataset(model, is_finetuned=False):\n",
    "\tresults = []\n",
    "\tfor row in validation_dataset:\n",
    "\t\tif is_finetuned:\n",
    "\t\t\tsystem_prompt = \"agent knows how to do SQL query\"\n",
    "\t\t\tprompt = row['question']\n",
    "\t\telse:\n",
    "\t\t\t#few shot learning\n",
    "\t\t\tsystem_prompt = \"Follow the user commands.\"\n",
    "\t\t\tprompt = f\"Translate the text command to SQL. Only output the SQL.\\n\" \\\n",
    "\t\t\t\t\t f\"Text: Return all data from the customers table  SQL: Select * from customers ###\\n\"\\\n",
    "\t\t\t\t\t f\"Text: Return all data from the customers table for customers' age > 60  SQL: Select * from customers where age > 60 ###\\n\"\\\n",
    "\t\t\t\t\t f\"Translate the text command to SQL. Only output the SQL.\\n\"\\\n",
    "\t\t\t\t\t f\"Text: {row['question']} SQL: \"\n",
    "\t\tresult = call_openai(system_prompt,prompt, model)\n",
    "\t\tresult = result.replace(\";\", \"\")\n",
    "\t\tresults.append(result)\n",
    "\n",
    "\tvalidation_df = pd.DataFrame(validation_dataset)\n",
    "\tvalidation_df['predicted_query'] = results\n",
    "\tvalidation_df = reformating_data(validation_df)\n",
    "\tvalidation_df.to_csv(f\"{model}_validation_accuracy_results.csv\", index=False)\n",
    "\taccuracy = (validation_df['predicted_query'] == validation_df['query']).mean()\n",
    "\tprint(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to OpenAI for SFT(supervised fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai():\n",
    "\tvalidation_file = client.files.create(\n",
    "\t\tfile=open(\"validation_dataset.jsonl\", \"rb\"),\n",
    "\t\tpurpose=\"fine-tune\"\n",
    "\t)\n",
    "\ttraining_file = client.files.create(\n",
    "\t\tfile=open(\"training_dataset.jsonl\", \"rb\"),\n",
    "\t\tpurpose=\"fine-tune\",\n",
    "\t)\n",
    "\treturn training_file, validation_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI SFT(supervised fine tuning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_openai_model(training_file, validation_file):\n",
    "\tresults = client.fine_tuning.jobs.create(\n",
    "\t\ttraining_file=training_file.id,\n",
    "\t\tvalidation_file=validation_file.id,\n",
    "\t\tmodel=\"gpt-3.5-turbo\",\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance few shot learning(w/o finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(model = llm_model)\n",
    "format_finetuning_dataset()\n",
    "evaluate_dataset(model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of fine-tuned model: 70.00%\n"
     ]
    }
   ],
   "source": [
    "training_file, validation_file = upload_dataset_to_openai()\n",
    "train_openai_model(training_file, validation_file)\n",
    "evaluate_dataset(model=\"ft:gpt-3.5-turbo-0125:personal::AEyiOkSI\", is_finetuned=True)\n",
    "data = pd.read_csv(\"ft:gpt-3.5-turbo-0125:personal::AEyiOkSI_validation_accuracy_results.csv\")\n",
    "print(f\" Accuracy of fine-tuned model: {np.sum(data['query'] == data['predicted_query'])/len(data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL query generation using finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Follow the user commands.\"\n",
    "prompt = \"Find the name of the employee who has the highest salary from table employee.\"\n",
    "model = \"ft:gpt-3.5-turbo-0125:personal::AEyiOkSI\"\n",
    "call_openai(system_prompt,prompt, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other evaluation metric\n",
    "\n",
    "### As the output generated is string exact match of special characters like puctuation,spaces may not exactly match with the reference(truth).In that case following are a few evaluation metric to detect its functional accuracy or entailment etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# %pip install evaluate\n",
    "\n",
    "# %pip install bert_score\n",
    "# %pip install rouge_score\n",
    "bertscore = load(\"bertscore\") # from paper https://arxiv.org/pdf/1904.09675.pdf\n",
    "bleu = load(\"bleu\") # from paper https://aclanthology.org/P02-1040.pdf\n",
    "rouge = load(\"rouge\") # from paper https://aclanthology.org/W04-1013.pdf\n",
    "entailment_pipe = pipeline(\"text-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statement:  ['select count(*) from singer'] \n",
      "Reference:  ['select count(*) from singer']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 7, 'reference_length': 7}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [0.9999999403953552], 'recall': [0.9999999403953552], 'f1': [0.9999999403953552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9697027802467346}\n",
      "\n",
      "Statement:  ['select count(*) from singer'] \n",
      "Reference:  ['select count(*) from singer']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 7, 'reference_length': 7}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [0.9999999403953552], 'recall': [0.9999999403953552], 'f1': [0.9999999403953552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9697027802467346}\n",
      "\n",
      "Statement:  ['select name , country , age from singer order by age desc'] \n",
      "Reference:  ['select name , country , age from singer order by age desc']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 12, 'reference_length': 12}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9404878616333008}\n",
      "\n",
      "Statement:  ['select name , country , age from singer order by age desc'] \n",
      "Reference:  ['select name , country , age from singer order by age desc']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 12, 'reference_length': 12}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9404878616333008}\n",
      "\n",
      "Statement:  [\"select avg(age) , min(age) , max(age) from singer where country = 'france'\"] \n",
      "Reference:  [\"select avg(age) , min(age) , max(age) from singer where country = 'france'\"]\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 21, 'reference_length': 21}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [0.9999998807907104], 'recall': [0.9999998807907104], 'f1': [0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.969047486782074}\n",
      "\n",
      "Statement:  [\"select avg(age) , min(age) , max(age) from singer where country = 'france'\"] \n",
      "Reference:  [\"select avg(age) , min(age) , max(age) from singer where nationality = 'french'\"]\n",
      "Bleu {'bleu': 0.8769536014223438, 'precisions': [0.9047619047619048, 0.85], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 21, 'reference_length': 21}\n",
      "Rouge {'rouge1': 0.8333333333333334, 'rouge2': 0.8181818181818182, 'rougeL': 0.8333333333333334, 'rougeLsum': 0.8333333333333334}\n",
      "Bert Score {'precision': [0.9862723350524902], 'recall': [0.9862723350524902], 'f1': [0.9862723350524902], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'not_entailment', 'score': 0.7438386678695679}\n",
      "\n",
      "Statement:  ['select song_name , song_release_year from singer order by age limit 1'] \n",
      "Reference:  ['select singer_name , year from songs order by cast(year as unsigned) desc limit 1']\n",
      "Bleu {'bleu': 0.39976376279504494, 'precisions': [0.6470588235294118, 0.3125], 'brevity_penalty': 0.8890097654027757, 'length_ratio': 0.8947368421052632, 'translation_length': 17, 'reference_length': 19}\n",
      "Rouge {'rouge1': 0.6428571428571429, 'rouge2': 0.23076923076923075, 'rougeL': 0.5714285714285715, 'rougeLsum': 0.5714285714285715}\n",
      "Bert Score {'precision': [0.9258612990379333], 'recall': [0.8916535973548889], 'f1': [0.908435583114624], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'not_entailment', 'score': 0.9869511127471924}\n",
      "\n",
      "Statement:  ['select song_name , song_release_year from singer order by age limit 1'] \n",
      "Reference:  ['select song_name , release_year from singer where singer in (select singer_id from singer order by singer_birth_date desc limit 1)']\n",
      "Bleu {'bleu': 0.34182454223666764, 'precisions': [0.8823529411764706, 0.6875], 'brevity_penalty': 0.43887992979155477, 'length_ratio': 0.5483870967741935, 'translation_length': 17, 'reference_length': 31}\n",
      "Rouge {'rouge1': 0.6111111111111112, 'rouge2': 0.4705882352941177, 'rougeL': 0.6111111111111112, 'rougeLsum': 0.6111111111111112}\n",
      "Bert Score {'precision': [0.9359015226364136], 'recall': [0.8961560130119324], 'f1': [0.915597677230835], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'not_entailment', 'score': 0.808946967124939}\n",
      "\n",
      "Statement:  ['select distinct country from singer where age > 20'] \n",
      "Reference:  ['select distinct country from singer where age > 20']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 9, 'reference_length': 9}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9791889786720276}\n",
      "\n",
      "Statement:  ['select distinct country from singer where age > 20'] \n",
      "Reference:  ['select distinct country from singer where age > 20']\n",
      "Bleu {'bleu': 1.0, 'precisions': [1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 9, 'reference_length': 9}\n",
      "Rouge {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Bert Score {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.42.0)'}\n",
      "NLI Entailment {'label': 'entailment', 'score': 0.9791889786720276}\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.8769536014223438, 0.39976376279504494, 0.34182454223666764, 1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert</th>\n",
       "      <th>entailment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.861854</td>\n",
       "      <td>0.901587</td>\n",
       "      <td>0.981031</td>\n",
       "      <td>0.928754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       blue     rouge      bert  entailment\n",
       "0  0.861854  0.901587  0.981031    0.928754"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_results(generated_results,references):\n",
    "\t\tbleu_scores = []\n",
    "\t\trouge_scores = []\n",
    "\t\tbert_scores = []\n",
    "\t\tentailment_scores = []\t\n",
    "\t\tfor generated,reference in zip(generated_results,references):\n",
    "\t\t\tprint(\"\\nStatement: \", generated, \"\\nReference: \", reference)\n",
    "\t\t\tresults = bleu.compute(predictions=generated, references=reference,max_order=2)\n",
    "\t\t\tprint(\"Bleu\", results)\n",
    "\t\t\tbleu_scores.append(results['bleu'])\n",
    "\n",
    "\t\t\tresults = rouge.compute(predictions=generated, references=reference)\n",
    "\t\t\tprint(\"Rouge\", results)\n",
    "\t\t\trouge_scores.append(results['rougeL'])\n",
    "\t\t\t\n",
    "\t\t\tresults = bertscore.compute(predictions=generated, references=reference, lang=\"en\")\n",
    "\t\t\tprint(\"Bert Score\", results)\n",
    "\t\t\tbert_scores.append(results['f1'][0])\n",
    "\n",
    "\t\t\tresult = entailment_pipe({'text':generated, 'text_pair': reference})\n",
    "\t\t\tprint(\"NLI Entailment\", result)\n",
    "\t\t\tentailment_scores.append(result['score'])\n",
    "\t\tprint(bleu_scores)\t\t\n",
    "\t\tsummary=pd.DataFrame()\n",
    "\t\tsummary = pd.DataFrame({'blue':[np.mean(bleu_scores)]}) \n",
    "\t\tsummary['rouge']=np.mean(rouge_scores)\n",
    "\t\tsummary['bert']=np.mean(bert_scores)\n",
    "\t\tsummary['entailment']=np.mean(entailment_scores)\n",
    "\t\treturn summary\n",
    "\n",
    "\n",
    "generated_results = [[data['query'][i]] for i in range(len(data))]\n",
    "references = [[data['predicted_query'][i]] for i in range(len(data))]\n",
    "calculate_results(generated_results, references)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
